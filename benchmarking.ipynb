{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DevCloud for Edgeを使ったベンチマークの方法を理解する\n",
    "OpenVINOには多くのサンプルプログラムやデモプログラムが添付されています。サンプルプログラムの多くはOpenVINOでできることを示すための機能デモであったり、OpenVINO APIの使用方法を示すためのサンプルプログラムであり、Performance benchmarkには向きません。  \n",
    "その中で、benchmark_appというサンプルプログラムはベンチマークを取ることに特化して作られたプログラムです。  \n",
    "benchmark_appにはさまざまなオプションが指定できますが、オプションを省略した場合でも指定されたハードウエアに最適なオプションを自動で適用してベンチマークを実行してくれるので便利です。\n",
    "- 指定されたDLアクセラレータに最適なオプションを自動で適用してくれる\n",
    "- 非同期推論を行い、高スループットが出せる条件で計測できる\n",
    "- おおよそどんなDLモデルでもベンチマーク可能\n",
    " - benchmark_appは入力も出力も気にせず、純粋にDLモデルを処理する時間を計測する\n",
    " - 推論結果をパースしない。出力blobがいくつあっても、どんなフォーマットでもOK\n",
    " - 入力データを必要としない。入力blobがいくつあっても、入力データが用意できなくてもOK\n",
    "- Python版とC++版が用意されている\n",
    "\n",
    "#### ここでは、OpenVINOのベンチマークツール(Python版)を使って基本的なDevCloud for Edgeでのbenchmarkの取り方を学びます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは次のことを学びます:\n",
    "- benchmark_appを**host server system**上で実行する方法\n",
    "- ジョブを送信し、benchmark_appを**いづれかのedge computing node**上で実行する方法\n",
    "- ジョブを送信し、benchmark_appを**特定のedge computing node**上で実行する方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 1. Development Server上でのbenchmark_appの実行の仕方\n",
    "ここではPython版のbenchmark_appを使用するのでビルドなどは不要ですぐに実行することができます。(C++版もあります)  \n",
    "ベンチマークを行う対象のDeep Learningモデルが必要となりますが、ここではSqueezenet1.1をダウンロードして使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. benchmark_appをコピーする\n",
    "`benchmark_app.py`はOpenVINOのインストールディレクトリ内にあります。毎回長いパス名を打つのは手間がかかるので、カレントディレクトリにコピーしてきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!cp -r $INTEL_OPENVINO_DIR/deployment_tools/tools/benchmark_tool .\n",
    "!ls -l benchmark_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. benchmark_appに必要なPython moduleをインストールする\n",
    "`reqirements.txt`に記載されているbenchmark_appの依存Pythonモジュールをインストールします。この作業は最初の1回だけ必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r benchmark_tool/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Neural network modelをダウンロードする\n",
    "`squeezenet 1.1`のモデルを、OpenVINO付属の`Model downloader`を使ってダウンロードします。`Model downloader`は指定されたモデルをネットワーク経由でダウンロードしてくれます。ダウンロード可能なモデルのリストは`--print_all`オプションで確認可能です。<br>\n",
    "ダウンロードされたsqueezenetモデルはCaffe形式のモデルなので**OpenVINOで利用できるIRモデル形式に変換する必要があります。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeezenet1.1モデルをダウンロード (Caffe形式)\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --name squeezenet1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) Model downloaderでダウンロード可能なOMZ DLモデルを一覧表示 (OMZ=Open Model Zoo, OpenVINO model zoo)\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. ダウンロードしたCaffe形式のsqueezenetモデルをIRモデルに変換する\n",
    "通常、Caffe, TensorFlow, ONNX, MxNetなどのモデルをIRモデルに変換するには、OpenVINO付属の`Model Optimizer`というツールを使用します。<br>\n",
    "しかしながら、**`Model downloader`でダウンロードしたモデルに関しては`Model converter`を使うことで簡単にIR形式への変換を行うことが可能です。**`Model converter`は`Model Optimizer`のフロントエンドツールで、内部では適切なオプションをつけて`Model optimizer`を実行しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/converter.py --name squeezenet1.1 --precisions FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. 変換されたIRモデルを確認する\n",
    "IRモデルは`.xml`と`.bin`で構成されます。`.xml`がトポロジー(グラフ)、`.bin`がウエイト＋バイアスデータです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l public/squeezenet1.1/FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. benchmark_appをDevelopment serverマシンで実行してみる\n",
    "まずはEdge inference compute nodeを使用せず、**Development host上でbenchmark_appを実行してみます。**Development serverは今現在皆さんがログインし操作しているマシンです。Development serverはXeonプラットフォームなので内蔵GPUがありません。またVPUやFPGAといったアクセラレータも搭載していませんので、**推論に使用できるデバイスはCPUのみ**となります。  \n",
    "Development host上で実行する場合ジョブの送信なども不要なので簡単に実行できますし、ジョブがキューで待たされることもありません。ローカルのマシンで普通にOpenVINOのアプリケーションを実行するのと同じように実行可能です。  \n",
    "実行結果ログの下のほうにパフォーマンスデータが表示されますので確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "benchmark_app options:  \n",
    "- `-m` specify the IR model file (.xml)\n",
    "- `-niter` specify the number of inference in the benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 benchmark_tool/benchmark_app.py \\\n",
    "        -m public/squeezenet1.1/FP16/squeezenet1.1.xml \\\n",
    "        -niter 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 2. benchmark_appをEdge inference nodeで実行してみる\n",
    "ジョブを送信し、先ほど実行したbenchmark_appをedge compute node上で実行してみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 送信するジョブスクリプトを作成する\n",
    "ここでは実際に送信するジョブ内容を記述したジョブスクリプト(普通のシェルスクリプト)ファイルを`%%writefile`マジックコマンドで作成します。`%%writefile`マジックコマンドは後に続くセル内の内容をすべてファイルに書き出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job.sh\n",
    "cd ~/devcloud-workshop-jp\n",
    "pip3 install -r benchmark_tool/requirements.txt\n",
    "python3 benchmark_tool/benchmark_app.py \\\n",
    "        -m public/squeezenet1.1/FP16/squeezenet1.1.xml \\\n",
    "        -niter 100 \\\n",
    "        $*\n",
    "echo completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ジョブスクリプトをEdge computing nodeに送信し、終了を待つ\n",
    "ここではBasic編でも使ったジョブ終了待ちのためのPythonコードを使用しています。  \n",
    "また、`qsub`コマンドでedge computing nodeを指定せずにジョブを送信しているので、ベンチマークはedge nodeの「どれか」で実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit a job\n",
    "job_id=!qsub job.sh\n",
    "\n",
    "# generate log file name from job_id\n",
    "job_num = job_id[0].split('.')[0]\n",
    "log_file='job.sh.o'+job_num\n",
    "err_file='job.sh.e'+job_num\n",
    "print('job_id={}, log_file={}'.format(job_id, log_file))\n",
    "\n",
    "import time\n",
    "def waitForJobCompletion(jobNumber):\n",
    "    print('Waiting for job completion...', end='')\n",
    "    running=True\n",
    "    while running:\n",
    "        time.sleep(1)\n",
    "        running=False\n",
    "        status_list=!qstat         # Check job status\n",
    "        for status in status_list:\n",
    "            if jobNumber in status:    # if job_num is found in the status list, the job is still running\n",
    "                running = True\n",
    "        print(status.split()[4], end='')\n",
    "    print('...Job {} completed'.format(job_num))    \n",
    "\n",
    "# wait for the job to complete\n",
    "waitForJobCompletion(job_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ジョブの結果のログファイルを確認する\n",
    "実行されたノード名が`# Resources:`に、ベンチマークの結果が下のほうにありますのでログファイルをチェックします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1つ前のセルでlog_fileにログファイル名を入れている。環境変数にセットしてcatから使えるようにする\n",
    "import os\n",
    "os.environ['log_file']=log_file\n",
    "\n",
    "!cat $log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 3. コンピュートノードを指定してジョブを実行してみる\n",
    "先ほどはジョブを実行するエッジコンピュートノードを指定せずにジョブを送信しました。この場合、空いている任意のコンピュートノードでジョブが実行されます。パフォーマンスベンチマークなどを行う際にはジョブを割り当てるコンピュートノードに制限をつけ、特定の構成のコンピュートノードで実行されるようにコントロールする必要があります。  \n",
    "Edge compute nodeには様々なコンフィグレーションのハードウエアがそろっています。CPUもAtomからXeonまで、各種アクセラレータを積んだノードも用意されています。  \n",
    "利用可能なコンピュートノードを調べるには**`pbsnodes`**コマンドを使用します。ノード指定に必要なノード名は`properties =`の行に含まれますのでその行だけをフィルターして表示します。ノード名はコンマで区切られて複数の名前が並びます。たとえばノードとして`intel-hd-530`を指定した場合、`intel-hd-530`をpropertiesフィールドに含むいずれかノードでジョブが実行されます。厳密にノードを指定したい場合、より狭い名前を指定する必要があります。  \n",
    "たとえば`idc001skl`のような名前を指定すると特定のノードで実行することが可能ですが、指定したノードのジョブがたまっている場合、長い時間待たされることがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 利用可能なコンピュートノードの一覧を表示する\n",
    "まずは利用可能なコンピュートノード一覧を表示させてみましょう。`pbsnodes`コマンドを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep \"properties =\" | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ノードを指定してジョブを送る\n",
    "ここでは`qsub`コマンドで具体的な実行ノードを指定してベンチマークジョブを送信しています。また、`-F`オプションで`job.sh`に対するオプションを渡しています。\n",
    "```\n",
    "job_id=!qsub -l nodes=1:gold6138 job.sh -F \"-d CPU\"\n",
    "```\n",
    "- `qsub`コマンド\n",
    " - `qsub`コマンドでジョブを送るときに、`-l`(limit)オプションをつけることでジョブを実行するノードを制限(=指定)することができます。`-l nodes=1:node_name`のように指定します。たとえば、`skylake`ノードで実行したい場合、`qsub -l nodes=1:skylake job.sh`のように指定します。  \n",
    " - また、`-F`オプションをつけることでコマンド引数を渡すことも可能です。この場合、ジョブスクリプトが引数を受け取れるようにしておく必要があることに注意してください。  \n",
    " - 今回はジョブスクリプトは先に作成した`job.sh`をそのまま利用します。\n",
    "- ログファイル名\n",
    " -ここでは`qsub`コマンドから返される`job id` (`['27214.v-qsvr-1.devcloud-edge']`のような文字列のリスト)からログファイル名を自動生成させています\n",
    "- ジョブの終了検出\n",
    " - 前回は手動で`qstat`コマンドを何度も実行し、`job.sh`ジョブがなくなるのを確認しましたが、今回はPythonを使って自動で終了を検出するようにしています"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: benchmark_appコマンドの引数について\n",
    "benchmark_appには様々なオプションが指定可能です。`--help`オプションをつけることで使用可能なオプションを表示できますが、代表的なものを下記に示します。  \n",
    "ここでは`\"-d CPU\"`として`CPU`を推論デバイスに指定していますが、**`GPU`, `MYRIAD`など他のデバイスを指定することで様々な条件でベンチマークを行うことが可能です。その際には`qsub`コマンドの`-l`オプションで指定する推論デバイスが利用可能なEdge computing nodeを指定することを忘れないようにしてください。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Option|Description|\n",
    "|:--|:--|\n",
    "|`-m` PATH_TO_MODEL|推論に使用するIRモデルファイルを指定(`.xml`)|\n",
    "|`-d` TARGET_DEVICE|推論デバイスを指定。`CPU`, `GPU`, `MYRIAD`, `HDDL`, `HETERO:FPGA,CPU`などを指定可能|\n",
    "|`-niter` NUMBER_ITERATIONS|実行する推論数。省略すると1分間推論を行う|\n",
    "|`-nireq` NUMBER_INFER_REQUESTS|同時推論実行数。たとえば4を指定すると同じデバイスに推論要求を同時に4つ投げる。Throughputを上げるためにはデバイス特性に合った同時推論数を指定するのが肝要。省略するとbenchmark_appが自動的に推論デバイスに適切な同時推論数を使用|\n",
    "|`-b` BATCH_SIZE|バッチ推論数|\n",
    "|`-i` PATH_TO_INPUT|推論に使用する入力画像ファイルを指定。benchmark_appでは省略可能(入力データなしでもベンチマーク可能)|\n",
    "|`-pc`|レイヤーごとの詳細実行レポートを表示。レイヤーごとの実行時間も含まれる|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit a job\n",
    "job_id=!qsub -l nodes=1:gold6138 job.sh -F \"-d CPU\"\n",
    "\n",
    "# generate log file name from job_id\n",
    "print('job_id=', job_id)\n",
    "job_num = job_id[0].split('.')[0]\n",
    "log_file='job.sh.o'+job_num\n",
    "err_file='job.sh.e'+job_num\n",
    "print('log_file=', log_file)\n",
    "\n",
    "# wait for the job to complete\n",
    "waitForJobCompletion(job_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ジョブの結果を確認する\n",
    "前回は単純にログファイル全体を表示しましたが、今回は`grep`を使用して必要な部分だけを見やすく表示させています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['log_file']=log_file\n",
    "\n",
    "!grep '# Resources:'                                  $log_file\n",
    "!grep '\\[ INFO \\] Device info'                   -A 3 $log_file\n",
    "!grep '\\[Step 11/11\\] Dumping statistics report' -A 4 $log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "ここではbenchmark_appをDevCloudで走らせてベンチマークを取る方法について学びました。\n",
    "benchmark_appはどんなモデルでも走らせることが可能ですので、独自のDLモデルであってもDevCloudにアップロードし、簡単にベンチマークを取ることが可能です。  \n",
    "DevCloudのストレージは他のユーザーやインテルの管理者からも参照できないようになっていますので安心して利用することが可能です。\n",
    "\n",
    "## Next => [OpenVINOを使用するC++プロジェクトの作り方と、DevCloudでの実行方法](./cpp-project.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
